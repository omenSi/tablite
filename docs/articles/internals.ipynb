{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internals\n",
    "\n",
    "## How tablite works\n",
    "\n",
    "> To understand the limitations of any system you must first understands how it works.\n",
    "\n",
    "\n",
    "When analyzing large(r) datasets most analytical operations are perform on or across few columns.\n",
    "\n",
    "If the data hence is stored as rows, it means that every row needs to be read, the required columns filtered out, kept in memory and the results calculated.\n",
    "\n",
    "If the data, in contrast, is stored as columns, the data can immediately be accessed as a single stream that can be read and processed.\n",
    "\n",
    "Tablite's internal datastructure is column based, so filtering and selecting columns is a very fast.\n",
    "\n",
    "In our test suite, the [1TB test](https://github.com/root-11/tablite/blob/master/tests/test_filereader_time.py#L102) verifies that an order file with 9 BN rows can be sliced in less than a second. We call this tolerable.\n",
    "\n",
    "![1TB test](https://github.com/root-11/tablite/blob/master/docs/img/1TB_test.png)\n",
    "\n",
    "If the data was in ram in column-oriented format the integer column alone would claim 297,967,747,072 bytes (297 Gb) so there are not be many PCs that can handle this in RAM.\n",
    "\n",
    "Tablite uses the commonly available SolidStateDisk (SSD) as storage. SSD's typically deliver from 120 Mb/s and up in _linear read_. Linear read means that the data is available in a single stream. The column based format that tablite uses is hence exploiting the hardware in the best possible manner, with reading excess data that needs to be discarded, such as the row based format.\n",
    "\n",
    "Linear reads should be compared to random reads: Amazon's CTO Werner Vogels described this as follows: Imagine you need to copy you movie collection of around 1 Terabyte from disk to another. If you do this using random reads it will take you around 20 days. If you do it using linear reads you'll be done in 90 minutes.\n",
    "\n",
    "Tablite will therefore _appear_ \"SLOW\" if you're appending rows by rows. Tablite will _appear_ fast if you're doing column-by-column operations.\n",
    "\n",
    "## Zero entropy = Zero copy\n",
    "\n",
    "Tablite's data is organised as follows:\n",
    "\n",
    "- Tables are dicts that point to Columns.\n",
    "- Columns are wrappers for columnar operations and point to a PageHandler\n",
    "- A PageHandlers manages low level data operations and have pointers to Pages\n",
    "- Pages take care of datatype conversion and contain pointers to the HDF5 storage where the data is stored.\n",
    "\n",
    "When concatenating two tables like in this operation: `table3 = table1 + table2`, the new Table that tablite creates (`table3`) will only create copies of the _references_ to the Pages that contain data. It does not copy the data. That is also why Tablite only will claim a few extra bytes in RAM and on SSD. The same applies to `table.copy()`. If there is no new information (entropy) there is no need to copy.\n",
    "\n",
    "When you perform list comprehensionsm, such as this: `Table['a'] = [i for i in range(10,000,000)]`, Tablite will perform 10 IOP as the max page size is 1,000,000 entries.\n",
    "\n",
    "Tablite Tables are mutable, and should you change a value, such as `Table['a'][3] = 7` the memory_manager will create a new page with the updates. Any references that point towards the \"old page\" will remain unchanged.\n",
    "\n",
    "This also gives insights to *why* Tablite is so quick in the 1 TB test: Tablite only needs to do 1 linear reads the first and the last page for each column. That 22 IOPS at and your SSD will probably permit around 100,000 IOPS.\n",
    "\n",
    "## Multiprocessing everywhere\n",
    "\n",
    "Tablite's collection of helpers use multiprocessing whereever possible. \n",
    "\n",
    "The exceptions to this rule are reading and writing `ods`, `xlsx` and `xls` files.\n",
    "\n",
    "For all other operations Tablite _in general_ performs the following operation:\n",
    "\n",
    "1. Analyse the dataset, using the metadata in ram, to determine the optimal multi-processing task size based on number of virtual CPUs and the free memory.\n",
    "2. Create a block of shared memory for task coordination. This is typically an index, such as a sort index. \n",
    "3. Launch subprocesses and queue tasks in a task queue.\n",
    "4. Let the subprocess work on the tasks, by:\n",
    "   1. Read the task limitations\n",
    "   2. Perform the required operation (for example sort a page according the index in shared memory)\n",
    "   3. Write the page to disk.\n",
    "5. Finally the subprocesses run out of work and exit.\n",
    "6. The main process assembles the output (a new table) using metadata only.\n",
    "\n",
    "This approach seems to work well when there's a lot of data. When the datasets have fewer than 1,000,000 fields (columns * rows), it doesn't pay off to start the subprocesses and tablite simply performs the operation in the main process.\n",
    "\n",
    "\n",
    "## Best practices?\n",
    "\n",
    "Manage your IOPs to get the most out of Tablite\n",
    "\n",
    "- Avoid usage row-by-row operation as each row requires 1 IO operation (IOP) and your SSD can probably max take 100,000 IO operations / second (IOPS).\n",
    "- Do use `Column.extend( lots of values )` as this appends \"lots of values\" in a 1 IO. \n",
    "- Do use list comprehensions whenever you require row by row operation, such as `Table['a'] = [i for i in range(10,000,000)]` . In this operation there are 10 IOP as the max page size is 1,000,000 entries.\n",
    "- Tables are mutable but there's really no need. Creating a new table only duplicates the coordinating metadata, so you may create new tables without having to worry about the volume of data. This: `table3 = table1 + table2` is only a few bytes in RAM and zero extra bytes on disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Will I run out of memory?\n",
    "\n",
    "You wont. Every tablite table is backed by HDF5 on disk. The memory footprint of a table is only the metadata required to know the relationships between variable names and the datastructures.\n",
    "\n",
    "Let's do a comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's monitor the memory and record the observations into a table!\n",
    "\n",
    "First we get the imports, Tablite Table and psutil for measuring memory, and we create a nice table for our measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\github\\tablite\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th></tr><tr><th>row</th><th>mixed</th><th>mixed</th><th>mixed</th></tr></table></div>"
      ],
      "text/plain": [
       "Table(3 columns, 0 rows)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "from tablite import Table\n",
    "import psutil, os, gc\n",
    "import array\n",
    "from time import process_time, sleep\n",
    "process = psutil.Process(os.getpid())\n",
    "baseline_memory = process.memory_info().rss\n",
    "\n",
    "digits = 1_000_000\n",
    "\n",
    "records = Table()\n",
    "records.add_column('method')\n",
    "records.add_column('memory')\n",
    "records.add_column('time')\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the common and convenient \"row\" based format and create 1 million lists with 10 integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = process_time()\n",
    "L = []\n",
    "for _ in range(digits):\n",
    "    L.append(tuple([11 for _ in range(10)]))\n",
    "end = process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check taskmanagers memory usage and add it to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th></tr><tr><th>row</th><th>str</th><th>int</th><th>float</th></tr><tr><td>0</td><td>1e6 lists w. 10 integers</td><td>139952128</td><td>0.5781</td></tr></table></div>"
      ],
      "text/plain": [
       "Table(3 columns, 1 rows)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.add_rows(('1e6 lists w. 10 integers', process.memory_info().rss - baseline_memory, round(end-start,4)))\n",
    "records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we're using ~140 Mb to store 1 million lists with 10 items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clear the list and let pythons garbage collector clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.clear()\n",
    "gc.collect()\n",
    "sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use a columnar format instead: We create 10 lists with 1 million integers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = process_time()\n",
    "L = [[11 for i in range(digits)] for _ in range(10)]\n",
    "end = process_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then we check psutil for memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th></tr><tr><th>row</th><th>str</th><th>int</th><th>float</th></tr><tr><td>0</td><td>1e6 lists w. 10 integers</td><td>139952128</td><td>0.5781</td></tr><tr><td>1</td><td>10 lists with 1e6 integers</td><td>86790144</td><td>0.3125</td></tr></table></div>"
      ],
      "text/plain": [
       "Table(3 columns, 2 rows)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.add_rows(('10 lists with 1e6 integers', process.memory_info().rss - baseline_memory, round(end-start,4)))\n",
    "records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we're using ~86 Mb to store the 10 lists with 1 million items in RAM.\n",
    "\n",
    "We now clear the list and let pythons garbage collector clean up again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.clear()\n",
    "gc.collect()\n",
    "sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've thereby saved 50 Mb by avoiding the overhead from managing 1 million lists.\n",
    "\n",
    "Q: But why didn't I just use an array? It would have even lower memory footprint.\n",
    "A: First, array's don't handle None's and we get that frequently in dirty csv data.\n",
    "\n",
    "Second, Table needs even less memory.\n",
    "\n",
    "Okay, let's start with an array just for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = process_time()\n",
    "L = [array.array('i', [11 for _ in range(digits)]) for _ in range(10)]\n",
    "end = process_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Again, we check psutils memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th></tr><tr><th>row</th><th>str</th><th>int</th><th>float</th></tr><tr><td>0</td><td>1e6 lists w. 10 integers</td><td>139952128</td><td>0.5781</td></tr><tr><td>1</td><td>10 lists with 1e6 integers</td><td>86790144</td><td>0.3125</td></tr><tr><td>2</td><td>10 lists with 1e6 integers in arrays</td><td>46510080</td><td>0.4375</td></tr></table></div>"
      ],
      "text/plain": [
       "Table(3 columns, 3 rows)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.add_rows(('10 lists with 1e6 integers in arrays', process.memory_info().rss - baseline_memory, round(end-start,4)))\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we're using 60.0 Mb to store 10 lists with 1 million integers in an array.\n",
    "\n",
    "Note that an array has to be a single data type that does not like anything else. This measurement is thereby a highly optimized case for single datatypes only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L.clear()\n",
    "gc.collect()\n",
    "sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's use Table:\n",
    "\n",
    "We will first create 10 columns with 1 million integers, followed by making a copy of that table and look at both results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th></tr><tr><th>row</th><th>str</th><th>int</th><th>float</th></tr><tr><td>0</td><td>1e6 lists w. 10 integers</td><td>139952128</td><td>0.5781</td></tr><tr><td>1</td><td>10 lists with 1e6 integers</td><td>86790144</td><td>0.3125</td></tr><tr><td>2</td><td>10 lists with 1e6 integers in arrays</td><td>46510080</td><td>0.4375</td></tr><tr><td>3</td><td>Table with 10 columns with 1e6 integers</td><td>6545408</td><td>1.5312</td></tr></table></div>"
      ],
      "text/plain": [
       "Table(3 columns, 4 rows)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = process_time()\n",
    "t = Table()\n",
    "for i in range(10):\n",
    "    t.add_column(str(i), data=[11 for _ in range(digits)])\n",
    "end = process_time()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "records.add_rows(('Table with 10 columns with 1e6 integers', process.memory_info().rss - baseline_memory, round(end-start,4)))\n",
    "records\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add a copy of the tablite Table, and measure again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th></tr><tr><th>row</th><th>str</th><th>int</th><th>float</th></tr><tr><td>0</td><td>1e6 lists w. 10 integers</td><td>139952128</td><td>0.5781</td></tr><tr><td>1</td><td>10 lists with 1e6 integers</td><td>86790144</td><td>0.3125</td></tr><tr><td>2</td><td>10 lists with 1e6 integers in arrays</td><td>46510080</td><td>0.4375</td></tr><tr><td>3</td><td>Table with 10 columns with 1e6 integers</td><td>6545408</td><td>1.5312</td></tr><tr><td>4</td><td>2 Tables with 10 columns with 1e6 integers each</td><td>6557696</td><td>0.0469</td></tr></table></div>"
      ],
      "text/plain": [
       "Table(3 columns, 5 rows)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = process_time()\n",
    "t2 = t.copy()\n",
    "end = process_time()\n",
    "\n",
    "records.add_rows(('2 Tables with 10 columns with 1e6 integers each', process.memory_info().rss - baseline_memory, round(end-start,4)))\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psutils now shows that we're using ~6.6 Mb to store 10 columns with 1 million integers for the first table, and only a few kilobytes more for the second table as only the metadata remains in pythons memory. Notice also how quick the copy is: 62.5 milliseconds.\n",
    "\n",
    "Let's just make that memory column a bit easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><table border=1><tr><th>#</th><th>method</th><th>memory</th><th>time</th><th>RAM (Mb)</th></tr><tr><th>row</th><th>str</th><th>int</th><th>float</th><th>str</th></tr><tr><td>0</td><td>1e6 lists w. 10 integers</td><td>139952128</td><td>0.5781</td><td>140.0 Mb</td></tr><tr><td>1</td><td>10 lists with 1e6 integers</td><td>86790144</td><td>0.3125</td><td>86.8 Mb</td></tr><tr><td>2</td><td>10 lists with 1e6 integers in arrays</td><td>46510080</td><td>0.4375</td><td>46.5 Mb</td></tr><tr><td>3</td><td>Table with 10 columns with 1e6 integers</td><td>6545408</td><td>1.5312</td><td>6.5 Mb</td></tr><tr><td>4</td><td>2 Tables with 10 columns with 1e6 integers each</td><td>6557696</td><td>0.0469</td><td>6.6 Mb</td></tr></table></div>"
      ],
      "text/plain": [
       "Table(4 columns, 5 rows)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records['RAM (Mb)'] = [ f\"{round(i/1e6,1)} Mb\" for i in records['memory'] ]\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion is comforting:\n",
    "\n",
    "- A drop from ~140 Mb to ~7 Mb working memory.\n",
    "- At this rate a projection of Tablite's Table class is roughly 18Mb per 1 Gb of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tablite310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b75c014ea7baa1f5b7be99940e6541d4a8a7395e27dcba8ac6dff802f822df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
